import os
import re
import time
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import sys

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

BASE_URL = "http://t66y.com"
LIST_URL = "https://t66y.com/thread0806.php?fid=8&type=1&page={page}"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

SAVE_DIR = "downloads"
os.makedirs(SAVE_DIR, exist_ok=True)

def fetch_page(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.encoding = "utf-8"
        return resp.text
    except Exception as e:
        logging.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
        return None

def parse_list_page(html):
    """è§£æåˆ—è¡¨é¡µï¼Œè¿”å› (æ ‡é¢˜, é“¾æ¥) åˆ—è¡¨"""
    soup = BeautifulSoup(html, "lxml")
    tbody = soup.find("tbody", id="tbody")
    results = []
    if not tbody:
        logging.warning("âš ï¸ æœªæ‰¾åˆ° tbody")
        return results
    rows = tbody.find_all("tr", class_="tr3 t_one tac")
    for row in rows:
        a_tag = row.find("h3").find("a") if row.find("h3") else None
        if not a_tag:
            continue
        title = a_tag.get_text(strip=True)
        href = urljoin(BASE_URL, a_tag["href"])
        results.append((title, href))
    return results

def parse_thread_page(html):
    """è§£æä¸»é¢˜é¡µï¼Œè¿”å›æ‰€æœ‰å›¾ç‰‡é“¾æ¥"""
    soup = BeautifulSoup(html, "lxml")
    img_tags = soup.find_all("input", src=True) + soup.find_all("img", src=True)
    urls = []
    for tag in img_tags:
        src = tag["src"]
        if src.lower().endswith((".jpg", ".jpeg", ".png")):
            urls.append(src)
    return urls

def download_image(url, folder, idx):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        if resp.status_code == 200:
            ext = os.path.splitext(url)[-1].split("?")[0]
            if ext.lower() not in [".jpg", ".jpeg", ".png"]:
                ext = ".jpg"
            filename = os.path.join(folder, f"{idx}{ext}")
            with open(filename, "wb") as f:
                f.write(resp.content)
            logging.info(f"âœ… ä¸‹è½½æˆåŠŸ: {filename}")
    except Exception as e:
        logging.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")

def main(start_page, end_page):
    for page in range(start_page, end_page + 1):
        url = LIST_URL.format(page=page)
        logging.info(f"ğŸ“„ æŠ“å–åˆ—è¡¨é¡µ: {url}")
        html = fetch_page(url)
        if not html:
            continue
        threads = parse_list_page(html)
        logging.info(f"ç¬¬ {page} é¡µå…±æ‰¾åˆ° {len(threads)} ä¸ªä¸»é¢˜")

        for title, link in threads:
            logging.info(f"â¡ï¸ è¿›å…¥ä¸»é¢˜: {title} ({link})")
            html = fetch_page(link)
            if not html:
                continue
            img_urls = parse_thread_page(html)
            if not img_urls:
                logging.info("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡")
                continue
            # å»ºç«‹æ–‡ä»¶å¤¹ä¿å­˜
            safe_title = re.sub(r'[\\/*?:"<>|]', "_", title)
            folder = os.path.join(SAVE_DIR, safe_title)
            os.makedirs(folder, exist_ok=True)
            # ä¸‹è½½å›¾ç‰‡
            for idx, img_url in enumerate(img_urls, 1):
                download_image(img_url, folder, idx)
            time.sleep(1)

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print(f"ç”¨æ³•: python {sys.argv[0]} <start_page> <end_page>")
        sys.exit(1)
    start = int(sys.argv[1])
    end = int(sys.argv[2])
    main(start, end)
